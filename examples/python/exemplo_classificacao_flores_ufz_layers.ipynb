{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kptczy1Nr_Hb"
      },
      "outputs": [],
      "source": [
        "# bibliotecas\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.applications import InceptionV3\n",
        "from tensorflow.keras.applications.inception_v3 import preprocess_input\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix, classification_report\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# clonar repositório\n",
        "!git clone https://github.com/PDI20/pdi_2024_2025.git"
      ],
      "metadata": {
        "id": "nyhKpbpgsXSu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# unzip do dataset\n",
        "!unzip /content/pdi_2024_2025/dataset/flowers.zip"
      ],
      "metadata": {
        "id": "UXZTWyRYsgLG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = (299, 299)\n",
        "\n",
        "# carregar imagens\n",
        "train_set = tf.keras.utils.image_dataset_from_directory(\n",
        "    \"/content/flowers/train/\",\n",
        "    seed=1337,\n",
        "    image_size=input_size,\n",
        "    label_mode='categorical'\n",
        ")\n",
        "\n",
        "val_set = tf.keras.utils.image_dataset_from_directory(\n",
        "    \"/content/flowers/valid/\",\n",
        "    seed=1337,\n",
        "    image_size=input_size,\n",
        "    label_mode='categorical'\n",
        ")\n",
        "\n",
        "test_set = tf.keras.utils.image_dataset_from_directory(\n",
        "    \"/content/flowers/test/\",\n",
        "    seed=1337,\n",
        "    image_size=input_size,\n",
        "    label_mode='categorical'\n",
        ")\n",
        "\n",
        "# classes\n",
        "class_names = train_set.class_names"
      ],
      "metadata": {
        "id": "L6eBvBHOsx5-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "examples_per_class = {class_name: [] for class_name in class_names}\n",
        "\n",
        "# 5 imagens de cada classe\n",
        "for images, labels in train_set:\n",
        "    for img, label in zip(images, labels):\n",
        "        class_idx = tf.argmax(label).numpy()  # Convert one-hot encoded label to class index\n",
        "        class_name = class_names[class_idx]\n",
        "\n",
        "        # Only add examples if we have fewer than 5 for this class\n",
        "        if len(examples_per_class[class_name]) < 5:\n",
        "            examples_per_class[class_name].append(img)\n",
        "\n",
        "    # Stop if we have 5 examples for each class\n",
        "    if all(len(img_list) == 5 for img_list in examples_per_class.values()):\n",
        "        break\n",
        "\n",
        "# Plot\n",
        "fig, axs = plt.subplots(len(class_names), 5, figsize=(15, len(class_names) * 3))\n",
        "fig.suptitle(\"Exemplos\", fontsize=16)\n",
        "for row, class_name in enumerate(class_names):\n",
        "    for col in range(5):\n",
        "        axs[row, col].imshow(examples_per_class[class_name][col].numpy())\n",
        "        axs[row, col].axis(\"off\")\n",
        "        if col == 0:\n",
        "            axs[row, col].set_title(class_name)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.subplots_adjust(top=0.95)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "eyh74jM62wX-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# normalização das imagens\n",
        "train_set = train_set.map(lambda x, y: (x / 255.0, y))\n",
        "val_set = val_set.map(lambda x, y: (x / 255.0, y))\n",
        "test_set = test_set.map(lambda x, y: (x / 255.0, y))"
      ],
      "metadata": {
        "id": "X420_H7p0PWg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# inicializar modelo\n",
        "base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(299, 299, 3))\n",
        "\n",
        "# congelar todas as layers\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# descongelar as últimas 50\n",
        "for layer in base_model.layers[-50:]:\n",
        "    layer.trainable = True\n",
        "\n",
        "# adicionar uma nova layer de saída ao modelo, para classificar as classes corretas\n",
        "model = models.Sequential([\n",
        "    base_model,\n",
        "    layers.GlobalAveragePooling2D(),\n",
        "    layers.Dense(2, activation='softmax')  # 2 classes\n",
        "])\n",
        "\n",
        "# compilar modelo\n",
        "model.compile(optimizer=Adam(learning_rate=0.0001),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "ZZM9yz2BsdUm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Early stopping callback para parar o treino quando não existe melhoria da validation loss\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=5,               # pára o treino após 5 épocas\n",
        "    restore_best_weights=True # reverter os pesos do treino para os melhores obtidos\n",
        ")\n",
        "\n",
        "# Reduce learning rate callback para atualizar o learning rate quando não existe melhoria da validation loss\n",
        "reduce_lr = ReduceLROnPlateau(\n",
        "    monitor='val_loss',\n",
        "    factor=0.1,               # reduzir learning rate por um fator de 0.1\n",
        "    patience=3,               # atualiza após 3 épocas\n",
        "    min_lr=1e-10              # learning rate mínimo\n",
        ")\n",
        "\n",
        "model_checkpoint = ModelCheckpoint('/content/model_checkpoint.keras', save_best_only=True, monitor='val_loss', mode='min'),"
      ],
      "metadata": {
        "id": "nscVkUAatXSg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# treinar o modelo\n",
        "history = model.fit(train_set,\n",
        "                    batch_size = 32,\n",
        "                    epochs=100,\n",
        "                    validation_data=val_set,\n",
        "                    callbacks=[early_stopping, reduce_lr, model_checkpoint])"
      ],
      "metadata": {
        "id": "_AEJpfSHtYzm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# criar figura\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "# Accuracy plot\n",
        "axes[0].plot(history.history['accuracy'], label='train_accuracy')\n",
        "axes[0].plot(history.history['val_accuracy'], label='val_accuracy')\n",
        "axes[0].set_xlabel('Epoch')\n",
        "axes[0].set_ylabel('Accuracy')\n",
        "axes[0].legend()\n",
        "axes[0].set_title('Model Accuracy')\n",
        "\n",
        "# Loss plot\n",
        "axes[1].plot(history.history['loss'], label='train_loss')\n",
        "axes[1].plot(history.history['val_loss'], label='val_loss')\n",
        "axes[1].set_xlabel('Epoch')\n",
        "axes[1].set_ylabel('Loss')\n",
        "axes[1].legend()\n",
        "axes[1].set_title('Model Loss')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TEVg-scJ1Cer"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# avaliar treino com base nos dados de teste\n",
        "test_loss, test_accuracy = model.evaluate(test_set)\n",
        "print(f\"Test accuracy: {test_accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "hpCrEJ7ytew2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_confusion_matrix(model, test_set, class_names):\n",
        "\n",
        "    # labels e previsões\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "\n",
        "    for images, labels in test_set:\n",
        "        predictions = model.predict(images)\n",
        "        y_pred.extend(np.argmax(predictions, axis=1))  # Previsão da classe\n",
        "\n",
        "\n",
        "        if labels.shape[-1] > 1:\n",
        "            y_true.extend(np.argmax(labels, axis=1))\n",
        "        else:\n",
        "            y_true.extend(labels.numpy())\n",
        "\n",
        "    # lista para array\n",
        "    y_true = np.array(y_true)\n",
        "    y_pred = np.array(y_pred)\n",
        "\n",
        "    # Generate classification report\n",
        "    print(\"\\n Relatório de Classificação:\")\n",
        "    print(classification_report(y_true, y_pred, target_names=class_names))\n",
        "\n",
        "    # matriz de confusão\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    # matrix de confusão\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
        "    disp.plot(cmap=\"Blues\", xticks_rotation=45)\n",
        "    plt.title(\"Matriz de Confusão\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "GaKH6vZ4nlhS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_confusion_matrix(model, test_set, class_names)"
      ],
      "metadata": {
        "id": "rirycE8opUeR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_path = \"\"\n",
        "\n",
        "# Load the image and resize to the model's expected input size\n",
        "img = load_img(image_path, target_size=target_size)\n",
        "\n",
        "# Convert the image to an array and normalize (to range [0, 1] if needed)\n",
        "img_array = img_to_array(img) / 255.0  # Normalize as done in training\n",
        "\n",
        "# Add an extra batch dimension since the model expects a batch of images\n",
        "img_array = np.expand_dims(img_array, axis=0)\n",
        "\n",
        "# Predict using the model\n",
        "predictions = model.predict(img_array)\n",
        "\n",
        "# Get the predicted class index\n",
        "predicted_class_idx = np.argmax(predictions, axis=1)[0]\n",
        "predicted_class = class_names[predicted_class_idx]\n",
        "confidence = predictions[0][predicted_class_idx]\n",
        "\n",
        "# Print results\n",
        "print(f\"Predicted class: {predicted_class} with confidence: {confidence * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "_vg7kjNvpe1p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# inicializar modelo\n",
        "base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(299, 299, 3))\n",
        "base_model.trainable = False  # congelar o modelo\n",
        "\n",
        "# adicionar uma nova layer de saída ao modelo, para classificar as classes corretas\n",
        "model_load_weights = models.Sequential([\n",
        "    base_model,\n",
        "    layers.GlobalAveragePooling2D(),\n",
        "    layers.Dense(2, activation='softmax')  # 2 classes\n",
        "])\n",
        "\n",
        "# compilar modelo\n",
        "model_load_weights.compile(optimizer=Adam(learning_rate=0.0001),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model_load_weights.build(input_shape=(None, 299, 299, 3))\n",
        "\n",
        "# carregar pesos\n",
        "model_load_weights.load_weights(\"/content/model_checkpoint.keras\")"
      ],
      "metadata": {
        "id": "B94NM1-L0-r8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the path and input size\n",
        "image_path = \"/content/images_test/test.jpg\"\n",
        "input_size = (299, 299)  # Define the input size used during training\n",
        "\n",
        "# Load the image and resize to the model's expected input size\n",
        "img = load_img(image_path, target_size=input_size)\n",
        "\n",
        "# Convert the image to an array and normalize (to range [0, 1] if needed)\n",
        "img_array = img_to_array(img) / 255.0  # Normalize as done in training\n",
        "\n",
        "# Add an extra batch dimension since the model expects a batch of images\n",
        "img_array = np.expand_dims(img_array, axis=0)\n",
        "\n",
        "# Predict using the model\n",
        "predictions = model.predict(img_array)\n",
        "\n",
        "# Get the predicted class index\n",
        "predicted_class_idx = np.argmax(predictions, axis=1)[0]\n",
        "predicted_class = class_names[predicted_class_idx]\n",
        "confidence = predictions[0][predicted_class_idx]\n",
        "\n",
        "# Print results\n",
        "print(f\"Predicted class: {predicted_class} with confidence: {confidence * 100:.2f}%\")\n",
        "\n",
        "# Display the image with prediction\n",
        "plt.imshow(img)\n",
        "plt.title(f\"Predicted: {predicted_class} ({confidence * 100:.2f}%)\")\n",
        "plt.axis(\"off\")  # Hide axes for a cleaner look\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Zb0nU6R3RrQZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}